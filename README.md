# üî¨ Protein‚ÄìProtein Interaction (PPI) Prediction  
### *A rigorous, production-grade pipeline combining classical ML, deep learning on embeddings, and LoRA-fine-tuned protein language models.*

---

## üìå **TL;DR ‚Äî What This Repository Achieves**
- Implements **strict protein-disjoint cross-validation** (no transductive leakage).  
- Trains **three families of models**:  
  1. Classical ML on ESM-2 embeddings  
  2. Neural models on frozen embeddings (MLP, Siamese, Transformer)  
  3. **LoRA fine-tuning of ProtBERT-BFD and ESM-2** using a **bi-encoder architecture**  
- Reaches **ROC-AUC ‚âà 0.80‚Äì0.88** on fully protein-disjoint splits.  
- Provides reproducible scripts for data generation, feature extraction, training, and evaluation.

---

# üß¨ 1. Background & Motivation

Protein‚Äìprotein interactions (PPIs) underlie virtually all cellular processes. As sequence-only computational prediction improves, protein language models (PLMs) have emerged as powerful tools for capturing structural and functional information without explicit structural data.

**Goal of this project:**  
> Given two raw protein sequences \( A \) and \( B \), predict whether they interact (binary classification).

This project was developed as part of a bioinformatics technical challenge. It recreates a **modern PPI prediction pipeline** using transformer-based PLMs and strict evaluation methodology.

---

# üö® 2. Avoiding Data Leakage: Protein-Disjoint CV

### Why normal random splitting is wrong
In most PPI datasets, proteins appear in many pairs:

```
Train: (A, X), (B, Y), (A, Z)  
Test:  (A, Q)
```

Even though the pairs differ, the **identity of protein A leaks into the test set** ‚Äî embeddings of A are already seen during training.

This leads to **inflated performance**.

### ‚úî The correct split: protein-disjoint CV

We split **proteins**, not pairs:

```
Proteins: [P1, P2, P3, ..., Pn]
             |            |
         Train IDs    Test IDs

Train pairs = (A, B) only if A ‚àà TrainIDs AND B ‚àà TrainIDs
Test pairs  = (A, B) only if A ‚àà TestIDs  AND B ‚àà TestIDs
```

This ensures:

- ‚úÖ No protein identity is ever seen twice across splits  
- ‚úÖ Models must generalize to truly unseen proteins  
- ‚úÖ Results simulate real-world discovery tasks  

We recommend **3-fold** CV for PPI datasets of ~2‚Äì3K pairs.

---

# üì¶ 3. Dataset Construction

### 3.1 Inputs
- `ppi_human_interactions.fasta` ‚Äî positive pairs  
- `ppi_negative_interactions.fasta` ‚Äî random negatives (generated)

Each FASTA line encodes:
```
>ID_A ID_B
SEQA-SEQB
```

### 3.2 Generating Negative Pairs
Negative pairs are sampled randomly **after removing all positive interactions** and **self-pairs**.

This is safe **only because protein-disjoint splitting eliminates SI leakage**.

Command:
```bash
python generate_negative_pairs.py --seed 42
```

### 3.3 Preparing Final Folds

This script:

- Parses sequences
- Extracts ESM-2 embeddings (cached)
- Extracts handcrafted features
- Produces protein-disjoint folds
- Saves everything under `curated_data/`

Run:

```bash
python prepare_dataset.py --n-folds 3 --device cuda
```

---

# üèó 4. Modeling Approaches

This repository evaluates three model families, from simple to advanced.

## ‚≠ê 4.1 Model 1 ‚Äî Classical ML on embeddings

**Features:**

- ESM-2 mean-pooled embeddings (1280-dim)
- Handcrafted 75-dim biological descriptors:
  - Amino acid composition
  - Hydrophobicity
  - Charge
  - Molecular weight
  - N-terminal, middle, C-terminal segmentation

**Classifiers:**

- Logistic Regression
- Random Forest
- XGBoost
- LightGBM
- KNN

**Typical Performance:**

| Model      | ROC-AUC    | Notes          |
|------------|------------|----------------|
| XGBoost    | 0.75‚Äì0.78  | Strong baseline|
| LightGBM   | 0.76‚Äì0.88  | Most stable    |

**Run:**

```bash
python models/benchmark.py --classifier XGBoost
```

## ‚≠ê 4.2 Model 2 ‚Äî Neural Architectures on Frozen Embeddings

Models operate on static ESM-2 embeddings.

**Variants:**

1. MLP with LayerNorm + GELU
2. Siamese MLP (shared encoder ‚Üí combine embeddings)
3. Transformer-encoder classifier

**Performance:** ~0.77‚Äì0.88 ROC-AUC, slightly above classical ML.

**Run:**

```bash
python models/model.py --epochs 20 --device cuda
```

## ‚≠ê 4.3 Model 3 ‚Äî LoRA Fine-Tuned Protein Language Models (Best)

### ‚ö° Key Innovation

Instead of training only on frozen embeddings, we fine-tune the PLM itself using **LoRA** (Low-Rank Adaptation), which updates <1% of parameters.

### Architecture: Bi-Encoder (Siamese Transformer)

We fine-tune two copies of the protein encoder with shared weights:

```python
u = Encoder(SeqA)
v = Encoder(SeqB)

features = [u, v, |u‚àív|, u*v]
logits = MLP(features)
```

**Why not a cross-encoder?**

- ‚úÖ Bi-encoders generalize to unseen proteins
- ‚ùå Cross-encoders leak identity between folds
- ‚úÖ Bi-encoders enable scalable retrieval options

**Models fine-tuned:**

- `Rostlab/prot_bert_bfd` (best overall)
- `facebook/esm2_t30_150M_UR50D`

**Performance:**

| Model                  | ROC-AUC (disjoint) |
|------------------------|--------------------|
| ProtBERT-BFD + LoRA    | 0.80‚Äì0.88          |
| ESM-2 150M + LoRA      | 0.79‚Äì0.88          |

**Run:**

ProtBERT:
```bash
python models/lora_model_protbert.py --device cuda --batch-size 4 --n-folds 1
```

ESM-2:
```bash
python models/lora_model_esm2.py --device cuda --batch-size 4 --n-folds 1
```

---

# üìà 5. Final Comparison

A script aggregates the top performers from Models 1, 2, and 3 and plots:

- ROC-AUC ¬± standard deviation
- Precision, recall, PR-AUC
- Confusion matrices

**Run:**

```bash
python plot_final_comparison.py
```

**Output:** `plot/final_model_comparison.png`

### Performance Comparison Plots

![ROC-AUC Comparison](plot/roc_auc_comparison.png)
*Figure 1: Comparison of Classical Classifiers (Model 1)*

![Neural Variants](plot/model2_comparison.png)
*Figure 2: Comparison of Neural Architectures (Model 2)*

![Final Model Comparison](plot/final_model_comparison.png)
*Figure 3: Best Model Comparison - Top performers from each category (Classical ML with ESM-2, Classical ML with Handcrafted features, Neural Networks, and LoRA fine-tuning) with error bars showing standard deviation across folds*

---

# üóÇ 6. Repository Structure

```
converge-task/
‚îú‚îÄ‚îÄ Data/
‚îÇ   ‚îú‚îÄ‚îÄ ppi_human_interactions.fasta
‚îÇ   ‚îú‚îÄ‚îÄ ppi_negative_interactions.fasta
‚îÇ
‚îú‚îÄ‚îÄ prepare_dataset.py               # Protein-disjoint splits + feature extraction
‚îú‚îÄ‚îÄ generate_negative_pairs.py       # Balanced negative sampling
‚îú‚îÄ‚îÄ plot_final_comparison.py         # Generate final comparison plot
‚îÇ
‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îú‚îÄ‚îÄ benchmark.py                 # Classical ML (Model 1)
‚îÇ   ‚îú‚îÄ‚îÄ model.py                     # Neural nets (Model 2)
‚îÇ   ‚îú‚îÄ‚îÄ lora_model_protbert.py       # LoRA ProtBERT bi-encoder (Model 3a)
‚îÇ   ‚îú‚îÄ‚îÄ lora_model_esm2.py           # LoRA ESM-2 bi-encoder (Model 3b)
‚îÇ
‚îú‚îÄ‚îÄ Output/
‚îÇ   ‚îú‚îÄ‚îÄ curated_data/                # Prepared folds
‚îÇ   ‚îú‚îÄ‚îÄ cache/                       # Embedding caches
‚îÇ   ‚îú‚îÄ‚îÄ plot/                        # Generated figures
‚îÇ   ‚îî‚îÄ‚îÄ *_results.txt                # Detailed logs
‚îÇ
‚îî‚îÄ‚îÄ requirements.txt
```

---

# ‚öôÔ∏è 7. Installation

```bash
conda create -n ppi python=3.10 -y
conda activate ppi

pip install -r requirements.txt
```

Or manually:

```bash
pip install transformers peft accelerate torch biopython scikit-learn sentencepiece matplotlib numpy
```

---

# üîç 8. Limitations & Future Work

- ‚ö†Ô∏è Negative sampling remains imperfect, but protein-disjoint CV mitigates most issues.
- Models do not yet incorporate:
  - Multiple sequence alignment (MSA)
  - Structural embeddings (AlphaFold2/Foldseek)
  - Graph attention networks over residue contacts
- Could extend bi-encoder to:
  - Approximate nearest-neighbor retrieval
  - Two-tower contrastive pre-training
- ESM-2 LoRA may benefit from QR-LoRA or AdaLoRA.

---

# üßæ 9. References

- **ESM-2**: Lin et al., *Highly Accurate Protein Structure Prediction Using Transformer Protein Language Models*, 2023
- **ProtBERT-BFD**: Elnaggar et al., *ProtTrans: Toward Cracking the Language of Life's Code Through Self-Supervised Deep Learning and High Performance Computing*, 2021
- **LoRA**: Hu et al., *LoRA: Low-Rank Adaptation for Large Language Models*, 2021

---

# üèÅ 10. Summary

This repository implements a fully **leakage-free**, **scientifically rigorous**, and **modern** approach to PPI prediction using transformer PLMs.

The final **LoRA-fine-tuned ProtBERT-BFD bi-encoder** sets a new baseline for this task, achieving **ROC-AUC ~0.80‚Äì0.88** on strictly protein-disjoint splits.

---

## üìú License

MIT License

## üë§ Author

Developed as part of a bioinformatics technical challenge for Converge.
